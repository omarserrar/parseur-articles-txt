<article>
<preamble>Vanderwende_2007_Beyond SumBasic</preamble>
<titre>Information Processing and Management 43 (2007) 1606–1618 www.elsevier.com/locate/infoproman </titre>
<auteur> Beyond SumBasic: Task-focused summarization with </auteur>
<abstract>Abstract In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speciﬁc information request, or topic, stated by the user. The system we have designed to accomplish this task comprises four main components: a generic extractive summarization system, a topic-focusing component, sentence simpliﬁcation, and lexical expansion of topic words. This paper details each of these components, together with experiments designed to quantify their individual contributions. We include an analysis of our results on two large datasets commonly used to evaluate task-focused summarization, the DUC2005 and DUC2006 datasets, using automatic metrics. Additionally, we include an analysis of our results on the DUC2006 task according to human evaluation metrics. In the human evaluation of system summaries compared to human summaries, i.e., the Pyramid method, our system ranked ﬁrst out of 22 systems in terms of overall mean Pyramid score; and in the human evaluation of summary responsiveness to the topic, our system ranked third out of 35 systems. Ó 2007 Elsevier Ltd. All rights reserved. Keywords: Summarization; Multi-document summarization; Sentence simpliﬁcation; Lexical expansion; Query expansion; NLP  </abstract>
<introduction>1. Introduction In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speciﬁc information request, or topic, stated by the user. In response to this interest and to stimulate research in this area, the National Institute of Standards and Technology (NIST) conducted a series of workshops, the Document Understanding Conference2 (DUC), to provide large-scale common data sets for the evaluation of systems. Participants in DUC2005 and DUC2006 were provided with a topic and a set of relevant documents (newswire articles), and the task was *  Corresponding author. Tel.: +1 425 706 5560; fax: +1 425 936 7329. E-mail addresses: lucyv@microsoft.com (L. Vanderwende), hisamis@microsoft.com (H. Suzuki), chrisbkt@microsoft.com (C. Brockett), anenkova@stanford.edu (A. Nenkova). 1 Present address: Stanford University, Stanford, CA 94305, USA. 2 http://duc.nist.gov. 0306-4573/$ - see front matter Ó 2007 Elsevier Ltd. All rights reserved. doi:10.1016/j.ipm.2007.01.023 </introduction>
<corp>L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606–1618  1607  to produce an automatic summary of not more than 250 words in length. Consider the topic description for clusters D0652G and D0655A, for example: D0652G: Identify the world’s top banana producers and their levels of production. Describe the main markets for imports and relate any major issues in the industry. D0655A: Discuss the causes, eﬀects, and treatment of childhood obesity. How widespread is it? In order to evaluate the summaries produced by the participants’ systems, called peer summaries, DUC provides four human summaries, model summaries, for comparison. Several methods for summarization evaluation have been proposed. The automatic metric used in DUC is ROUGE (Lin, 2004) speciﬁcally, ROUGE-2, which calculates the overlap in bigrams between the peer and the four model summaries, and ROUGE-SU4, which calculates the bigram overlap, but allowing up to 4 words to be skipped in order to identify a bigram match. Automatic metrics are useful as they potentially allow a comparison between diﬀerent system settings, as we show in Section 6.1. However, since we are primarily interested in maximizing the content of our system’s summaries, which, due to many issues of semantic realization such as paraphrase, cannot always be captured by measuring bigram overlap with four model summaries, we also evaluate our results with human evaluation metrics. Human evaluation is time-consuming and must be conducted carefully, in the same experimental setting, in order to ensure comparison across systems. We participated in DUC2006 in order to gain access to a human evaluation of our system, speciﬁcally the NIST content responsiveness score. In addition, a second human evaluation is coordinated by Columbia University, the Pyramid method (Nenkova et al., 2004). The Pyramid method requires two steps: ﬁrst, a set of semantic equivalence classes are built for the sets of model summaries, with higher scores assigned to content represented in multiple model summaries, and second, a person identiﬁes the content units in a peer summary that are found in the set of semantic equivalence classes.3 The scores reported measure the amount of content overlap between the peer summary and the four model summaries. In this paper, we describe the multi-document summarization system we submitted to DUC2006. Our contribution in DUC2006, identiﬁed as System 10, builds on an earlier system, SumBasic (Nenkova & Vanderwende, 2005) which produces generic multi-document summaries; we provide a description of SumBasic in Section 2. We then describe each of the remaining three main components that comprise our system: a task-focused extractive summarization system, sentence simpliﬁcation, and lexical expansion of topic words. We will provide experiments using automatic metrics designed to quantify the contributions of each component. Human evaluation metrics, discussed in Section 6.2, indicate that this is a relatively successful approach to multi-document summarization; in the Pyramid evaluation, our system ranked ﬁrst out of 22 systems and in the NIST metrics for content responsiveness, our system ranked third out of 35 systems. With regard to our system design, it must be noted that this system, similar to almost all multi-document summarization systems, produces summaries by selecting sentences from the document set, either verbatim or with some simpliﬁcation. Using sentence simpliﬁcation is a step towards generating new summary text, rather than extracting summaries from existing text. There is, however, no consideration for sentence ordering or cohesion other than that sentence ordering is determined exclusively as a result of the sentence selection process (see Section 2 for details). 2. Core system: SumBasic SumBasic (Nenkova & Vanderwende, 2005) is a system that produces generic multi-document summaries. Its design is motivated by the observation that words occurring frequently in the document cluster occur with higher probability in the human summaries than words occurring less frequently. Speciﬁcally, SumBasic uses the following algorithm: 3  In order to have summaries evaluated according to the Pyramid method, each participant was required to volunteer annotation eﬀort, which we did. In order to further contribute to the community eﬀort, Microsoft Research assisted in the creation of the semantic equivalence classes. Creation of the semantic equivalence tasks, which requires the model summaries, was carried out after ﬁnal test results had been submitted, and therefore, we did not have access to the model summaries at any time prior to the test.  1608  L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606–1618  Step 1. Compute the probability distribution over the words wi appearing in the input, p(wi) for every i; pðwi Þ ¼ Nn , where n is the number of times the word appeared in the input, and N is the total number of content word tokens in the input. Step 2. For each sentence Sj in the input, assign a weight equal to the average probability of the words in the sentence, i.e., WeightðS j Þ ¼  X Wi2Sj  pðwiÞ : jfwijwi 2 Sjgj  Step 3. Pick the best scoring sentence that contains the highest probability word. Step 4. For each word wi in the sentence chosen at step 3, update their probability: pnew ðwi Þ ¼ pold ðwi Þ  pold ðwi Þ: Step 5. If the desired summary length has not been reached, go back to Step 2. Steps 2 and 3 enforce the desired properties of the summarizer, i.e., that high frequency words from the input are very likely to appear in the human summaries. Step 3 ensures that the highest probability word is included in the summary, thus each time a sentence is picked, the word with the highest probability at that point in the summary is also picked. Step 4 serves a threefold purpose: 1. It gives the summarizer sensitivity to context. The notion of ‘‘what is most important to include in the summary?’’ changes depending on what information has already been included in the summary. In fact, while pold(wi) can be considered as the probability with which the word wi will be included in the summary, pnew(wi) is an approximation of the probability that the word wi will appear in the summary twice. 2. By updating the probabilities in this intuitive way, we also allow words with initially low probability to have higher impact on the choice of subsequent sentences. 3. The update or word probability gives a natural way to deal with the redundancy in the multidocument input. No further checks for duplication seem to be necessary. The system resembles SUMavr as recently described in Nenkova, Vanderwende, and McKeown (2006) except that the update function in SumBasic uses squaring rather than multiplication by a very small number. Comparing SumBasic to Luhn (1958). The idea that simple frequency is indicative of importance in automatic summarization dates back to the seminal work of Luhn (1958). The SumBasic algorithm is distinct from Luhn’s algorithm, however, in several signiﬁcant but illustrative ways. Luhn ﬁrst collects the frequencies of words in the document and identiﬁes a subset of signiﬁcant words, excluding the most frequent (what would now be termed ‘‘stopwords’’) and the least frequent (generally, words occurring less than four times). Whereas SumBasic uses true initial probabilities and computes the weight of a sentence as equal to the average probability of the words in a sentence, Luhn treats all signiﬁcant words as having equal weight and computes the weight of a sentence as a function of the concentration of signiﬁcant words in the sentence. This weight is obtained by deﬁning windows as sequences of signiﬁcant and non-signiﬁcant words, such that there are no more than four non-signiﬁcant words between any two signiﬁcant words in a window; the weight of a sentence is equal to the weight of the highest scoring window, namely, the square root of the number of signiﬁcant words in the window, divided by the total number of words in the window. Finally, since Luhn’s system is designed to summarize single-documents, there is little need to prevent redundancy, in sharp contrast to multidocument summarization, where the likelihood that several documents might convey highly similar, or even identical, important information necessitates mechanisms to avoid redundancy, a functionality that SumBasic provides by updating the probability of the words on the basis of preceding selected sentences. 3. SumFocus Nenkova and Vanderwende (2005) document the impact of frequency on generic multi-document systems, but in the case of task-focused summarization, document frequency alone may not be predictive, since the  L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606–1618  1609  topic may not be related to the most common information in the document set. Thus, in order to incorporate topic constraints in generic summarization, our new system, called SumFocus, captures the information conveyed by the topic description by computing the word probabilities of the topic description. Having done so, the weight for each word is computed as a linear combination of the unigram probabilities derived from the topic description, with backoﬀ smoothing to assign words not appearing in the topic a very small probability, and the unigram probabilities from the document, in the following manner (all other aspects of SumBasic remain unchanged): WordWeight ¼ ð1  kÞ  DocWeight þ k  TopicWeight: The optimal value of k, 0.9, was empirically determined using the DUC2005 corpus, manually optimizing on ROUGE-2 scores (henceforth R-2). Since sentence selection is controlled by choosing the words with the highest weights, it is, in principle, possible for these ‘‘best words’’ to come from either the document or from the topic description. In practice, however, the best word is nearly always a word from the topic description due to the high value assigned to k. For DUC2005 overall, 618 document words were identiﬁed as best on the basis of topic statements and only 22 independently on the basis of frequency alone. For DUC2006 overall, all 600 best words were chosen from the topic statements. On the basis of DUC2005 data, we added a small list of ‘‘topic stopwords’’ (describe, discuss, explain, identify, include, including, involve, involving) that did not receive any weight. The topic statements in DUC2006 appear to contain more instructions to the summarizer than in DUC2005, suggesting additional words may warrant similar handling (e.g., concerning, note, specify, give, examples, and involved). 4. Sentence simpliﬁcation Our goal is to create a summarization system that produces summaries with as much content as possible that satisﬁes the user, given a set limit on length.4 Since summaries produced by SumFocus alone are extractive, we view sentence simpliﬁcation (also known as sentence shortening or sentence compression) as a means of creating more space within which to capture important content. 4.1. Approaches to simpliﬁcation The most common approach to sentence simpliﬁcation for summarization purposes has been to deterministically shorten the sentences selected to be used in the summary. The CLASSY system (Conroy, Schlesinger, & Goldstein Stewart, 2005) for example, incorporates a heuristic component for sentence simpliﬁcation that preprocesses the sentences used in their sentence selection component. Columbia University’s summarization system uses a syntactic simpliﬁcation component (Siddharthan, Nenkova, & McKeown, 2004) the results of which are sent to their sentence clustering component. Daumé and Marcu (2005a) employ a post-processing approach and report that deleting adverbs and attributive phrases improve ROUGE scores in the Multilingual Summarization Evaluation, although this post-processing was not found to be useful in DUC2005 (Daumé & Marcu, 2005b) conceivably because the summaries are 250 words long rather than 100 words long. In these approaches, simpliﬁcation operations apply to all sentences equally, and the core sentence selection component has only either the original or the shortened sentence available to choose from. This may not be optimal, however, because the best simpliﬁcation strategy is not necessarily the same for all sentences. For example, it might be desirable to delete material X from a sentence only if X is already covered by another sentence in the summary. For this reason, simpliﬁcation strategies have so far remained conservative, presumably to avoid possible side-eﬀects of oversimpliﬁcation. An alternative approach to sentence simpliﬁcation is to provide multiple shortened sentence candidates for the summarization engine to choose from. Multi-Document Trimmer (Zajic, Dorr, Lin, Monz, & Schwartz, 2005) for instance uses a syntactic simpliﬁcation engine (Dorr, Zajic, & Schwartz, 2003) initially developed for headline generation, to output multiple simpliﬁed versions of the sentences in the document cluster. Each 4  For DUC2005 and DUC2006, the summary length was no more than 250 words.  1610  L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606–1618  Table 1 Syntactic patterns for sentence simpliﬁcation (underlined parts are removed) Pattern  Example  Noun appositive Gerundive clause  One senior, Liz Parker, had slacked oﬀ too badly to graduate The Kialegees, numbering about 450, are a landless tribe, sharing space in Wetumka, Okla., with the much larger Creek Nation, to whom they are related  Nonrestrictive relative clause  The return to whaling will be a sort of homecoming for the Makah, whose real name which cannot be written in English means ‘‘people who live by the rocks and the seagulls’’  Intra-sentential attribution Lead adverbials and conjunctions  Separately, the report said that the murder rate by Indians in 1996 was 4 per 100,000, below the national average of 7.9 per 100,000, and less than the white rate of 4.9 per 100,000  of these candidates are submitted to the feature-based sentence selection component, which includes the redundancy score of the sentence given the current state of the summary and the number of trimming operations as features. 4.2. Simpliﬁed sentences as alternatives Our approach to sentence simpliﬁcation most closely resembles that used in the Multi-Document Trimmer (Zajic et al., 2005): we apply a small set of heuristics to a parse tree to create alternatives, after which both the original sentence and (possibly multiple) simpliﬁed versions are available for selection. Unlike MDT, however, in our system both original and alternative simpliﬁed sentences are provided for selection without diﬀerentiation, i.e., without retaining any explicit link between them, under the assumption that the SumBasic-based multi-document summarization engine is inherently equipped with the ability to handle redundancy, and the simpliﬁed alternatives only add to that redundancy. SumBasic’s method for updating the unigram probabilities given the sentences already selected allows the simpliﬁed sentence alternatives to be considered independently, while maintaining redundancy at a minimum.5 Given that this approach to sentence simpliﬁcation allows the sentence selection component to make the optimal decision among alternatives, we are thus freed to pursue more aggressive simpliﬁcation, since the original non-simpliﬁed version is always available for selection. The approach is also extensible to incorporating novel sentence rewrites into a summary, moving in the direction of generative rather than extractive summaries. An example of this is Jing and McKeown (2000) who propose a set of operations to edit extracted sentences, not limited to sentence reduction. It is straightforward to integrate such sentence rewrite operations into our framework, as candidate generation works independently of sentence selection, and word probability alone suﬃces to compute the sentence score. 4.3. The syntax-based simpliﬁcation ﬁlter Our simpliﬁcation component consists of heuristic templates for the elimination of syntactic units based on parser output. Each sentence in the document cluster is ﬁrst parsed using a broad-coverage English parser (Ringger, Moore, Charniak, Vanderwende, & Suzuki, 2004). We then run a ﬁlter on the parse tree that eliminates certain nodes from the parse tree when the node matches the patterns provided heuristically. Table 1 lists the syntactic patterns used in our DUC2006 system. These patterns are inspired by and similar to those discussed in Dunlavy et al. (2003) the principal diﬀerence being that extraction makes use of a full-ﬂedged syntactic parser rather than employing a shallow parsing approach. For the ﬁrst three patterns in Table 1 (noun appositive, gerundive clause and non-restrictive relative clause), the parser returns a node label corresponding exactly to these patterns; we simply deleted the nodes with these labels. For the identiﬁcation of intra-sentential attribution, we added speciﬁc conditions for detecting the verbs of attribution (said in Table 1), its subject (the report), the complementizer (that) and adverbial expressions if any, and deleted the nodes when conditions were matched. In the case of sentence-initial adverbials, we delete only manner and time adverb expressions, 5  Note, however, that the probability update by SumBasic must be computed based on the original document cluster.  L. Vanderwende et al. / Information Processing and Management 43 (2007) 1606–1618  1611  using the features returned by the parser. Currently, these patterns all apply simultaneously to create maximally one simpliﬁed sentence per input, but it would in principle be possible to generate multiple simpliﬁed candidates. Finally, the punctuation and capitalization of the simpliﬁed sentences are cleaned up before the sentences are made available to the selection component along with their original, non-simpliﬁed counterparts in the document cluster. The results of applying this simpliﬁcation ﬁlter are discussed in Section 6. 5. Lexical expansion In addition to sentence simpliﬁcation, we also investigated the potential for expanding the task terms with synonyms and morphologically-related forms. The use of query expansion has frequently been explored in Information Retrieval tasks, but without notable success (Mitra, Singhal, & Buckley, 1998). However, since the purpose of summarization is not to extract entire relevant documents from a large data set, but smaller sentences, it might be hypothesized that individual expansions could have more evident impact. In constructing our system, therefore, we augmented the task terms with lexical expansions supplied by morphological variants (chieﬂy derived forms) and synonyms or closely-related terms drawn from both static hand-crafted thesauri and dynamically-learned sources extracted from corpora. Lexical expansions are applied only at the point where we choose the inventory of ‘‘best words’’ with which to determine candidate sentence selection. As already noted in Section 3, in DUC2006, the ‘‘best words’’ are drawn only from the topic statements, and not from the document collection. Where a term in the sentence matches a lexical expansion, the formula for computing ‘‘best word’’ scores is as follows, where d is the document score of the lexical item in question, and e is the score for each type of matched expansion: n X Expansion Score ¼ ð1  kÞ  d þ k ei : Pn  i  The summation i ei is the expanded analog of the TopicWeight in the formula given for unexpanded topics in </corp>
<conclusion></conclusion>
<references>References Brockett, C., & Dolan, W. B. (2005). Support vector machines for paraphrase identiﬁcation and corpus construction. In Proceedings of the third international workshop on paraphrasing (IWP2005), Jeju, Republic of Korea. Conroy, J. M., Schlesinger, J., & Goldstein Stewart, J. (2005). CLASSY query-based multi-document summarization. In Proceedings of DUC 2005. Daumé, H., III, & Marcu, D. (2005a). Bayesian multi-document summarization at MSE. In Proceedings of MSE 2005. Daumé, H., III, & Marcu, E. (2005b). Bayesian summarization at DUC and a suggestion for extrinsice evaluation. In Proceedings of DUC 2005. Dolan, W. B., Quirk, C., & Brockett, C. (2004). Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of COLING 2004, Geneva, Switzerland. Dorr, B., Zajic, D., & Schwartz, R. (2003). Hedge trimmer: a parse-and-trim approach to headline generation. In Proceedings of HLTNAACL 2003 text summarization workshop (pp. 1–8). Dunlavy, D., Conroy, J., Schlesinger, J., Goodman, S., Okurowski, M., O’Leary, E., et al. (2003). Performance of a three-stage system for multi-document summarization. In Proceedings of DUC 2003. Fellbaum, C. (Ed.). (1998). WordNet: An electronic lexical database. The MIT Press. Jing, H., & McKeown, K. (2000). Cut and past based text summarization. In Proceedings of the 1st conference of the North American Chapter of the Association for Computational Linguistics (NAACL’00). Lin, C. Y. (2004). ROUGE: a package for automatic evaluation of summaries. In Proceedings of the workshop on text summarization branches out, 25–26 July 2004, Barcelona, Spain. Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2), 159–165. Mitra, M., Singhal, A., & Buckley, C. (1998). Improving automatic query expansion. In Proceedings of the 21st Annual International ACMSIGIR conference on research and development in information retrieval. Moore, R. C. (2001). Towards a simple and accurate statistical approach to learning translation relationships among words. In Proceedings, workshop on data-driven machine translation, Toulouse, France. Moore, R. C. (2004). Association-based bilingual word alignment. In Proceedings, workshop on building and using parallel texts: datadriven machine translation and beyond, Ann Arbor, MI. Nenkova, A., & Vanderwende, L. (2005). The impact of frequency on summarization. MSR-TR-2005-101. Nenkova, A., Vanderwende, L., & McKeown, K. (2006). A compositional context sensitive multidocument summarizer. In Proceedings of SIGIR 2006. Nenkova, A., & Passonneau, R. (2004). Evaluating content selection in summarization: the Pyramid method. In Proceedings of the HLTNAACL 2004. Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), 19–52. Quirk, C., Brockett, C., & Dolan, W. B. (2004). Monolingual machine translation for paraphrase generation. In Proceedings of the 2004 conference on empirical methods in natural language processing, 25–26 July 2004, Barcelona Spain (pp. 142–149). Ringger, E., Moore, R. C., Charniak, E., Vanderwende, L., & Suzuki, H. (2004). Using the Penn treebank to evaluate non-treebank parsers. In Proceedings of LREC 2004. Rooney, K. (2001). Encarta Thesaurus. Bloomsbury Publishing. </references>
</article>
