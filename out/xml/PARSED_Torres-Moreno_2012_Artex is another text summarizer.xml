<article>
<preamble>Torres-Moreno_2012_Artex is another text summarizer</preamble>
<titre>Artex is AnotheR TEXt summarizer Juan-Manuel Torres-Moreno1,2 </titre>
<auteur>1 </auteur>
<abstract>Abstract This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that Artex summarizer achieves interesting results.  Keywords: Automatic Text Summarization, Space Vector Model, Text extraction, Ultrastemming  1  </abstract>
<introduction>Introduction  Automatic Text Summarization (ATS) is the process to automatically generate a compressed version of a source document [15]. Query-oriented summaries focus on a user’s request, and extract the information related to the specified topic given explicitly in the form of a query [2]. Generic mono-document summarization tries to cover as much as possible the information content. Multi-document summarization is a oriented task to create a summary from a heterogeneous set of documents on a focused topic. Over the past years, extensive experiments on query-oriented multi-document summarization have been carried out. Extractive Summarization produces summaries choosing a subset of representative sentences from original documents. Sentences are ordered and then assembled according to their relevance to generate the final summary [10]. This article introduces a new method of summarization based in sentences extraction on Vector Space Model (VSM). We score each sentence by calculating their inner product with a pseudo-sentence vector and a pseudo-word vector. Results show that Artex not only preserves the content of the summaries generated using this new representation, but often, surprisingly the performance can be improved. Artex could be an interesting and simple algorithm using the extractive summarization paradigm. Our tests on trilingual corpora (English, Spanish and French) evaluated by the Fresa algorithm (without human references) confirm the good performance of Artex. In this paper, related work is given in Section 2. Section 3 presents the new algorithm of Automatic Text Summarization. Experiments are presented in Section 4, followed by Results in Section 5 and Conclusions in Section 6. 1 </introduction>
<corp>2  Related works  Research in Automatic Text Summarization was introduced by H.P. Luhn in 1958 [9]. In the strategy proposed by Luhn, the sentences are scored for their component word values as determined by tf*idf-like weights. Scored sentences are then ranked and selected from the top until some summary length threshold is reached. Finally, the summary is generated by assembling the selected sentences in the original source order. Although fairly simple, this extractive methodology is still used in current approaches. Later on, [3] extended this work by adding simple heuristic features such as the position of sentences in the text or some key phrases indicate the importance of the sentences. As the range of possible features for source characterization widened, choosing appropriate features, feature weights and feature combinations have become a central issue. A natural way to tackle this problem is to consider sentence extraction as a classification task. To this end, several machine learning approaches that uses document-summary pairs have been proposed [6, 12], An hybrid method mixing statistical and linguistics algorithms is presented in [1]. [10] and [15] propose a good state-of-art of Automatic Text Summarization tasks and algorithms.  2.1  Document Pre-processing  The first step to represent documents in a suitable space is the pre-processing. As we use extractive summarization, documents have to be chunked into cohesive textual segments that will be assembled to produce the summary. Pre-processing is very important because the selection of segments is based on words or bigrams of words. The choice was made to split documents into full sentences, in this way obtaining textual segments that are likely to be grammatically corrects. Afterwards, sentences pass through several basic normalization steps in order to reduce computational complexity. The process is composed by the following steps: 1. Sentence splitting. Simple rule-based method is used for sentence splitting. Documents are chunked at the period, exclamation and question mark. 2. Sentence filtering. Words lowercased and cleared up from sloppy punctuation. Words with less than 2 occurrences (f < 2) are eliminated (Hapax legomenon presents once in a document). Words that do not carry meaning such as functional or very common words are removed. Small stop-lists (depending of language) are used in this step. 3. Word normalization. Remaining words are replaced by their canonical form using lemmatization, stemming, ultra-stemming or none of them (raw text). Four methods of normalization were applied after filtering: • Lemmatization by simples dictionaries of morphological families. These dictionaries have 1.32M, 208K and 316K words-entries in Spanish, English and French, respectively. • Porter’s Stemming, available at Snowball (web site http://snowball.tartarus. org/texts/stemmersoverview.html) for English, Spanish, French among other languages. • Ultra-stemming. This normalization seems be very efficient and it produces a compact matrix representation [16]. Ultra-stemming consider only the n first letters of  each word. For example, in the case of ultra-stemming (first letter, Fix1 ), inflected verbs like “sing”, “song”, “sings”, “singing”... or proper names “smith”, “snowboard”, “sex”,... are replaced by the letter “s”. 4. Text Vectorization. Documents are vectorized in a matrix S[P ×N ] of P sentences and N columns. Each element si,j represents the occurrences of an object j (a letter in the case of ultra-stemming, a word in the case of lemmatization or a stem for stemming), j = 1, 2, ..., N in the sentence i, i = 1, 2, ..., P .  3  AnotheR TEXt summarizer (Artex)  Artex1 is a simple extractive algorithm for Automatic Text Summarization. The main idea is the next one: First, we represent the text in a suitable space model (VSM). Then, we construct an average document vector that represents the average (the “global topic”) of all sentences vectors. At the same time, we obtain the “lexical weight” for each sentence, i.e. the number of words in the sentence. After that, it is calculated the angle between the average document and each sentence; narrow angles indicate that the sentences near of the “global topic” should be important and therefore extracted. See on the figure 1 the VSM of words: P vector sentences and the average “global topic” are represented in a N dimensional space of words.  w1  VSM words  s1  b Global topic  s2 α  si Sentence  wN  s3 wj  sP Angle cos α=(b • s )/||b|| ||s || i  i  Figure 1: The “global topic” in a Vector Space Model of N words. Next, a score for each sentence is calculated using their proximity with the “global topic” and their “lexical weight”. In the figure 2, the “lexical weight” is represented in a VSM of P sentences. Finally, the summary is generated concatenating the sentences with the highest scores following their order in the original document. 1 In  French, Artex est un Autre Résumeur TEXtuel.  VSM sentences  w1  s1 w2  a Lexical weight  wj  sP  wN si Figure 2: The “lexical weight” in a Vector Space Model of P sentences.  3.1  Algorithm  Formally, Artex algorithm computes the score of each sentence by calculating the inner product between a sentence vector, an average pseudo-sentence vector (the “global topic”) and an average pseudo-word vector (the “lexical weight”). Once a pre-processing (word normalization and filtering of stop words) is completed, it is created a matrix S[P ×N ] , using the Vector Space Model, that contains N words (or letters) and P sentences. Let si = (s1 , s2 , ..., sN ) be a vector of the sentence i, i = 1, 2, ..., P . We defined ~a the average pseudo-word vector, as the average number of occurrences of N words used in the sentence i: (1)  ai =  1 X si,j N j  and ~b the average pseudo-sentence vector as the average number of occurrences of each word j used trough the P sentences: (2)  bj =  1 X si,j P i  The score or weight of each sentence si is calculated as follows:  (3)      X 1  score(si ) = ~s × ~b × ~a = si,j × bj  × ai ; i = 1, 2, ..., P ; j = 1, 1, ..., N NP j  The score(•) computed by equation 3 must be normalized between the interval [0,1]. The calculation of ~s × ~b indicates the proximity between the sentence ~s and the average pseudosentence ~b. The product (~s × ~b) × ~a weigh this proximity using the average pseudo-word ai .  If a sentence si is near of ~b and their corresponding element ai has a high value, si will have, therefore, a high score. Moreover, a sentence i far of main topic (i.e. ~si × ~b is near 0) or a less informative sentence i (i.e. ai are near 0) will have a low score. In computational terms, it is not really necessary to divide the scalar product by the constant 1 ~ s/|~b||~s| between ~b and ~s is the same if we use ~b = ~b0 = P si,j . i N P , because the angle α = arccos b.~ The element ai is only a scale factor that does not modify α. 0 In fact, if the matrix S[P ×N ] is approximated to a binary matrix2 S[P ×N ] , where each element 1 s0 = {0, 1} has a probability of p = , we can normalize vectors ~a, ~b and matrix S, as follows: i,j  2  |~a| =  (4)  P q X  s0i,j 2 =  |~b| =  (5)  s0i,j 2 =  |~si | =  N q X j  N q X  ({0, 1}N )2 =  √ NP  j  j  (6)  √ ({0, 1}P )2 = N P  i  i N q X  P q X  s0i,j 2 =  N p X  {0, 1}2 = N  j  Vectors then will be represented in hyper-spheres of N or P dimensions, and the normalized score’ in this space would be:  score’(si )  (7)  =  =    ! X ~b ~a ~s 1 √  × si,j × bj  × ai × = √ |~s| |~b| |~a| N NPN P j   X 1  √ si,j × bj  × ai ; i = 1, 2, ..., P ; j = 1, 2, ..., N N 5P 3 j  √ However, the term 1/ N 5 P 3 is a constant value (i.e. a simple scale factor), and then the score(•) calculated using the equation 3) and the score’(•) using the equation 7, are both equivalent.  4  Experiments  Artex algorithm described in the previous section has been implemented and evaluated in corpora in several languages. We have conducted our experimentation with the following languages, summarization tasks, summarizers and data sets: 1) Generic multi-document-summarization in English with the corpus DUC’04; 2) Generic single-document summarization in Spanish with the corpus Medicina Clínica and 3) Generic single document summarization in French with the corpus Pistes. We have applied the summarization algorithms and finally, the results have been evaluated using Fresa while processing times for each summarizer have been measured and compared. The following subsections present formally the details of the summarizers, corpora and evaluations studied in different experiments. 2 This is a reasonable approximation in this context, because S [P ×N ] is a sparsed matrix with many term occurrences equal to one or zero.  4.1  Other Summarizers  To compare the performances, two other summarization systems were used in our experiments: Cortex and Enertex. To be in the same conditions, these two systems have used exactly the same textual representation based on Vector Space Model, described in Section 2.1. • Cortex is a single-document summarization system using several metrics and an optimal decision algorithm [4, 14, 15, 18]. • Enertex is a summarization system based in Textual Energy concept [5]: text is represented as a spin system where spins ↑ represents words that their occurrences are f > 1 (spins ↓ if the word is not present).  4.2  Summarization Corpora Description  To study the impact of our summarizer, we used corpora in three languages: English, Spanish and French. The corpora are heterogeneous, and different tasks are representatives of Automatic Text Summarization: generic multi-document summary and mono-document guided by a subject. • Corpus in English. Piloted by NIST in Document Understanding Conference3 (DUC) the Task 2 of DUC’044 , aims to produce a short summary of a cluster of related documents. We studied generic multi-document-summarization in English using data from DUC’04. This corpus with 300K words (17 780 types) is compound of 50 clusters, 10 documents each. • Corpus in Spanish. Generic single-document summarization using a corpus from the scientific journal Medicina Clínica5 , which is composed of 50 medical articles in Spanish, each one with its corresponding author abstract. This corpus contains 125K words (9 657 types). • Corpus in French. We have studied generic single-document summarization using the Canadian French Sociological Articles corpus, generated from the journal Perspectives interdisciplinaires sur le travail et la santé (Pistes)6 . It contains 50 sociological articles in French, each one with its corresponding author abstract. This corpus contains near 400K words (18 887 types).  4.3  Summaries Content Evaluation  DUC conferences have introduced the ROUGE content evaluation [7], wich measures the overlap of n-grams between a candidate summary and reference summaries written by humans. However, to write the human summaries necessaries for ROUGE is a very expensive task. </corp>
<conclusion></conclusion>
<references>and French) evaluated by the Fresa algorithm (without human references) confirm the good performance of Artex. In this paper, related work is given in Section 2. Section 3 presents the new algorithm of Automatic Text Summarization. Experiments are presented in Section 4, followed by Results in Section 5 and Conclusions in Section 6. 1 </references>
</article>
