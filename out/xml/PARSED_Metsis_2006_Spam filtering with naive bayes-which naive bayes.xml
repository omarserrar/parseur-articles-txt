<article>
<preamble>Metsis_2006_Spam filtering with naive bayes-which naive bayes</preamble>
<titre>Spam Filtering with Naive Bayes – Which Naive Bayes? Vangelis Metsis </titre>
<auteur> </auteur>
<abstract>ABSTRACT Naive Bayes is very popular in commercial and open-source anti-spam e-mail filters. There are, however, several forms of Naive Bayes, something the anti-spam literature does not always acknowledge. We discuss five different versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages. The new datasets, which we make publicly available, are more realistic than previous comparable benchmarks, because they maintain the temporal order of the messages in the two categories, and they emulate the varying proportion of spam and ham messages that users receive over time. We adopt an experimental procedure that emulates the incremental training of personalized spam filters, and we plot roc curves that allow us to compare the different versions of nb over the entire tradeoff between true positives and true negatives. </abstract>
<biblio>REFERENCES  [1] I. Androutsopoulos, J. Koutsias, K. Chandrinos, and C. Spyropoulos. An experimental comparison of Naive Bayesian and keyword-based anti-spam filtering with encrypted personal e-mail messages. In 23rd ACM SIGIR Conference, pages 160–167, Athens, Greece, 2000. [2] I. Androutsopoulos, G. Paliouras, and E. Michelakis. Learning to filter unsolicited commercial e-mail. technical report 2004/2, NCSR “Demokritos”, 2004. [3] R. Beckermann, A. McCallum, and G. Huang. Automatic categorization of email into folders: benchmark experiments on Enron and SRI corpora. Technical report IR-418, University of Massachusetts Amherst, 2004. [4] X. Carreras and L. Marquez. Boosting trees for anti-spam email filtering. In 4th International Conference on Recent Advances in Natural Language Processing, pages 58–64, Tzigov Chark, Bulgaria, 2001. [5] P. Domingos and M. Pazzani. On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning, 29(2–3):103130, 1997. [6] H. D. Drucker, D. Wu, and V. Vapnik. Support Vector Machines for spam categorization. IEEE Transactions On Neural Networks, 10(5):1048–1054, 1999. [7] S. Eyheramendy, D. Lewis, and D. Madigan. On the Naive Bayes model for text categorization. In 9th International Workshop on Artificial Intelligence and Statistics, pages 332–339, Key West, Florida, 2003. [8] T. Fawcett. In “vivo” spam filtering: a challenge  Enron2 - Multinomial NB, Boolean - 3000 Attributes  Enron1 - Multinomial NB, Boolean - 3000 Attributes  Enron3 - Multinomial NB, Boolean - 3000 Attributes  1  1  1  0.95  0.95  0.95  0.9  0.9  0.9  0.85  0.85  0.85  0.8  Spam Recall  0.75  Ham Recall  0.7  1  1  0.95  0.95  0.9  0.9  0.9  0.85  0.85  0.85 0.8 0.75  Spam Recall  Ham Recall  0.7  Number of emails x 100  Number of emails x 100  55  52  49  46  43  40  34  31  28  25  22  19  16  7  37  Ham Recall 58  55  52  49  46  43  40  37  34  31  28  25  22  19  16  49  46  43  40  37  34  31  28  25  22  19  16  13  10  7  7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58  4  4  1  1  Spam Recall  0.7 13  0.7  0.8 0.75  10  Ham Recall  13  Number of emails x 100  1  Spam Recall  Ham Recall  Enron6 - Multinomial NB, Boolean - 3000 Attributes  0.95  0.8  10  Number of emails x 100  Enron5 - Multinomial NB, Boolean - 3000 Attributes  Enron4 - Multinomial NB, Boolean - 3000 Attributes  0.75  4  58  55  52  49  46  43  40  37  34  31  28  25  22  19  16  7  13  10 13 16 19 22 25 28 31 34 37 40 43 46 49 Number of emails x 100  10  7  4  4  1  1  Spam Recall  0.7 1  0.7  0.8  0.75  7  Ham Recall  4  Spam Recall  1  0.8 0.75  Number of emails x 100  Figure 3: Learning curves for the multinomial NB with Boolean attributes and T = 0.5.  [9]  [10]  [11]  [12]  [13]  [14]  [15]  [16]  [17]  problem for KDD. SIGKDD Explorations, 5(2):140–148, 2003. S. Hershkop and S. Stolfo. Combining email models for false positive reduction. In 11th ACM SIGKDD Conference, pages 98–107, Chicago, Illinois, 2005. J. G. Hidalgo. Evaluating cost-sensitive unsolicited bulk email categorization. In 17th ACM Symposium on Applied Computing, pages 615–620, 2002. J. G. Hidalgo and M. M. Lopez. Combining text and heuristics for cost-sensitive spam filtering. In 4th Computational Natural Language Learning Workshop, pages 99–102, Lisbon, Portugal, 2000. J. Hovold. Naive Bayes spam filtering using word-position-based attributes. In 2nd Conference on Email and Anti-Spam, Stanford, CA, 2005. J. T. J.D.M. Rennie, L. Shih and D. Karger. Tackling the poor assumptions of Naive Bayes classifiers. In 20th International Conference on Machine Learning, pages 616–623, Washington, DC, 2003. G. John and P. Langley. Estimating continuous distributions in Bayesian classifiers. In 11th Conference on Uncertainty in Artificial Intelligence, pages 338–345, Montreal, Quebec, 1995. B. Klimt and Y. Yang. The Enron corpus: a new dataset for email classification research. In 15th European Conference on Machine Learning and the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases, pages 217–226, Pisa, Italy, 2004. A. Kolcz and J. Alspector. SVM-based filtering of e-mail spam with content-specific misclassification costs. In Workshop on Text Mining, IEEE International Conference on Data Mining, San Jose, California, 2001. A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In AAAI’98 Workshop on Learning for Text Categorization, pages 41–48, Madison, Wisconsin, 1998.  [18] E. Michelakis, I. Androutsopoulos, G. Paliouras, G. Sakkis, and P. Stamatopoulos. Filtron: a learning-based anti-spam filter. In 1st Conference on Email and Anti-Spam, Mountain View, CA, 2004. [19] P. Pantel and D. Lin. SpamCop: a spam classification and organization program. In Learning for Text Categorization – Papers from the AAAI Workshop, pages 95–98, Madison, Wisconsin, 1998. [20] F. Peng, D. Schuurmans, and S. Wang. Augmenting naive bayes classifiers with statistical language models. Information Retrieval, 7:317–345, 2004. [21] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A Bayesian approach to filtering junk e-mail. In Learning for Text Categorization – Papers from the AAAI Workshop, pages 55–62, Madison, Wisconsin, 1998. [22] G. Sakkis, I. Androutsopoulos, G. Paliouras, V. Karkaletsis, C. Spyropoulos, and P. Stamatopoulos. Stacking classifiers for anti-spam filtering of e-mail. In Conference on Empirical Methods in Natural Language Processing, pages 44–50, Carnegie Mellon University, Pittsburgh, PA, 2001. [23] G. Sakkis, I. Androutsopoulos, G. Paliouras, V. Karkaletsis, C. Spyropoulos, and P. Stamatopoulos. A memory-based approach to anti-spam filtering for mailing lists. Information Retrieval, 6(1):49–73, 2003. [24] K.-M. Schneider. A comparison of event models for Naive Bayes anti-spam e-mail filtering. In 10th Conference of the European Chapter of the ACL, pages 307–314, Budapest, Hungary, 2003. [25] K.-M. Schneider. On word frequency information and negative evidence in Naive Bayes text classification. In 4th International Conference on Advances in Natural Language Processing, pages 474–485, Alicante, Spain, 2004.  </biblio>
</article>
